{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71174a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp vad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb61c7c",
   "metadata": {},
   "source": [
    "# Perform Voice Activity Detection (VAD)\n",
    "\n",
    "We want to start with voice activity detection to make sure we are not cutting off words and sentences in the middle.\n",
    "This should improve transcription reliability and make both the quantization and T2S model training easier.\n",
    "\n",
    "**Usage:**  \n",
    "```\n",
    "python -m whisperspeech.vad https://huggingface.co/datasets/collabora/librilight-webdataset/resolve/main/librilight-large-wo6454-flac-000002.tar\n",
    "```\n",
    "\n",
    "You can pass in either a URL or a local file name. The result will go into a file in the current directory named after the source file but replacing `flac` with `vad` (check the `flac_to_vad_name` function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ffbdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ed2d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "from pathlib import Path\n",
    "from fastprogress import progress_bar, master_bar\n",
    "from fastcore.script import call_parse\n",
    "\n",
    "import numpy as np\n",
    "import webdataset as wds\n",
    "\n",
    "import whisperx\n",
    "\n",
    "from whisperspeech.inference import get_compute_device\n",
    "from whisperspeech import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6b2da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import pylab as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714048d3",
   "metadata": {},
   "source": [
    "We use the voice activity detection model from WhisperX (but we don't use their merging algorithm):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805e1bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def extract_segments(vad_result, max_duration):\n",
    "    binarize = whisperx.vad.Binarize(max_duration=max_duration)\n",
    "    segments = binarize(vad_result)\n",
    "    return [(x.start, x.end) for x in segments.get_timeline()]\n",
    "\n",
    "def segment_audio(vad_model, audio, sr=16000):\n",
    "    vad_result = vad_model({\"waveform\": audio, \"sample_rate\": sr})\n",
    "    return extract_segments(vad_result, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9110f217",
   "metadata": {},
   "source": [
    "## Batch processing\n",
    "\n",
    "Let's put everything above together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea03e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "# from https://huggingface.co/spaces/facebook/MusicGen/blob/9cae843238aad3f5c7695a40c9ee77c42dd87aaf/audiocraft/data/audio_utils.py\n",
    "def normalize_loudness(wav: torch.Tensor, sample_rate: int, loudness_headroom_db: float = 14,\n",
    "                       loudness_compressor: bool = False, energy_floor: float = 2e-3):\n",
    "    \"\"\"Normalize an input signal to a user loudness in dB LKFS.\n",
    "    Audio loudness is defined according to the ITU-R BS.1770-4 recommendation.\n",
    "    Args:\n",
    "        wav (torch.Tensor): Input multichannel audio data.\n",
    "        sample_rate (int): Sample rate.\n",
    "        loudness_headroom_db (float): Target loudness of the output in dB LUFS.\n",
    "        loudness_compressor (bool): Uses tanh for soft clipping.\n",
    "        energy_floor (float): anything below that RMS level will not be rescaled.\n",
    "    Returns:\n",
    "        torch.Tensor: Loudness normalized output data.\n",
    "    \"\"\"\n",
    "    torch.set_num_threads(8)\n",
    "    energy = wav.pow(2).mean().sqrt().item()\n",
    "    if energy < energy_floor:\n",
    "        return wav, 0\n",
    "    transform = torchaudio.transforms.Loudness(sample_rate)\n",
    "    input_loudness_db = transform(wav).item()\n",
    "    # calculate the gain needed to scale to the desired loudness level\n",
    "    delta_loudness = -loudness_headroom_db - input_loudness_db\n",
    "    gain = 10.0 ** (delta_loudness / 20.0)\n",
    "    output = gain * wav\n",
    "    if loudness_compressor:\n",
    "        output = torch.tanh(output)\n",
    "    assert output.isfinite().all(), (input_loudness_db, wav.pow(2).mean().sqrt())\n",
    "    return output, gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfc2a15-8d8b-444c-b860-3b48b293c995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "   \n",
    "def list_audio_files(tar_path):\n",
    "    audio_extensions = ['.flac', '.mp3', '.sox', '.wav', '.m4a', '.ogg', '.wma', '.opus']\n",
    "    audio_files = []\n",
    "    # Open the tar file\n",
    "    with tarfile.open(tar_path, 'r') as tar:\n",
    "        # Iterate through each member\n",
    "        for member in tar.getmembers():\n",
    "            # Check if the member is a file and has a supported extension\n",
    "            if member.isfile():\n",
    "                if any(member.name.lower().endswith(ext) for ext in audio_extensions):\n",
    "                    audio_files.append(member.name)\n",
    "    return audio_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cada0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "from math import ceil\n",
    "@call_parse\n",
    "def process_shard(\n",
    "    url:str,           # input shard URL/path\n",
    "    output:str,          # output shard URL/path\n",
    "    key:str='audio',     # string to replace with 'vad' in the shard name\n",
    "    model:str='whisperx' # VAD model to use (possible values: `whisperx` or `pyannote`)\n",
    "):  \n",
    "    filenames = list_audio_files(url)\n",
    "    files_count = len(filenames)\n",
    "    ds = wds.WebDataset(url).compose(\n",
    "        wds.decode(utils.torch_audio_opus),\n",
    "        utils.find_audio,\n",
    "    )\n",
    "    dl = torch.utils.data.DataLoader(ds, num_workers=1, batch_size=None)\n",
    "    \n",
    "    if model == 'whisperx':\n",
    "        vad_model = whisperx.vad.load_vad_model(get_compute_device())\n",
    "    elif model == 'pyannote':\n",
    "        from pyannote.audio import Pipeline\n",
    "        pyannote_vad = Pipeline.from_pretrained(\"pyannote/voice-activity-detection\")\n",
    "    \n",
    "    def calc_power(audio, sr, ts, te):\n",
    "        snd = audio[:,int(ts*sr):int(te*sr)]\n",
    "        return (snd*snd).mean().log()\n",
    "    \n",
    "    def chunk_audio(audio, chunk_size_sec, sr):\n",
    "        chunk_size_samples = chunk_size_sec * sr\n",
    "        total_chunks = (audio.shape[1] + chunk_size_samples - 1) // chunk_size_samples\n",
    "        return iter([audio[:, i * chunk_size_samples:(i + 1) * chunk_size_samples] for i in range(total_chunks)])\n",
    "\n",
    "    with utils.AtomicTarWriter(output) as sink:\n",
    "        mb = master_bar(range(files_count))\n",
    "        dl_iterator = iter(dl)\n",
    "        for i in mb:\n",
    "            s = next(dl_iterator)\n",
    "            audio, sr = s['audio']\n",
    "            ash = audio.shape\n",
    "            shift = audio.mean()\n",
    "            \n",
    "            # Normalize audio chunk by chunk (30 minutes chunks)\n",
    "            chunk_duration_sec = 1800  # 30 minutes in seconds\n",
    "            n_chunks = ceil(audio.shape[1]/(chunk_duration_sec*sr))\n",
    "           \n",
    "            normalized_chunks = []\n",
    "\n",
    "            chunks = []\n",
    "            for j in progress_bar(range(n_chunks+2), parent=mb):\n",
    "                if j == 0:\n",
    "                    mb.child.comment = filenames[i] + ' <b>calculating chunks</b>'\n",
    "                    chunks = chunk_audio(audio - shift, chunk_duration_sec, sr)\n",
    "                elif (j <= n_chunks ):\n",
    "                    chunk = next(chunks)\n",
    "                    mb.child.comment = filenames[i] + ' <b>normalizing '+ str(j) +' of '+ str(n_chunks) + ' chunks</b>'\n",
    "                    norm_chunk, gain = normalize_loudness(chunk, sr)\n",
    "                    normalized_chunks.append(norm_chunk)\n",
    "                else:\n",
    "                    mb.child.comment = filenames[i] + ' <b>applying VAD model</b>'\n",
    "                    audio = torch.cat(normalized_chunks, dim=1)\n",
    "                    if model == 'whisperx':\n",
    "                        segments = segment_audio(vad_model, audio, sr=sr)\n",
    "                    elif model == 'pyannote':\n",
    "                        segments = [(x.start, x.end)\n",
    "                                    for x in pyannote_vad({\"waveform\":audio,\"sample_rate\":sr}).get_timeline().support()]\n",
    "                    powers = [calc_power(audio, sr, ts, te) for ts, te in segments]\n",
    "                    sink.write({\n",
    "                        \"__key__\": s['__key__'],\n",
    "                        \"gain_shift.npy\": np.array([gain, shift], dtype=np.float32),\n",
    "                        \"vad.npy\": np.array(segments, dtype=np.float32),\n",
    "                        \"powers.npy\": np.array(powers, dtype=np.float32),\n",
    "                    })\n",
    "            del s\n",
    "            del chunks\n",
    "            del normalized_chunks\n",
    "            del audio"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
